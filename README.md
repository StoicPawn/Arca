ğŸ§  Milestone 1 â€” Modulo Percettivo Auto-Supervisionato (Adapters)
ğŸ“˜ Obiettivo generale

Costruire un sottosistema percettivo universale che, senza alcuna supervisione esterna o hard-coding semantico,
impari a riconoscere e rappresentare in modo coerente pattern basso-livello (bordi, colori, texture, ritmi, timbri, ecc.)
producendo un embedding numerico d-dimensionale comune a tutte le modalitÃ .

Questo modulo sarÃ  la base sensoriale del progetto piÃ¹ ampio (â€œbackbone di co-occorrenza / cervelloâ€).
Deve poter essere addestrato from scratch, replicato integralmente, e fornire rappresentazioni
stabili, invariate e informative da collegare successivamente al backbone concettuale.

âš™ï¸ Requisiti

Nessuna etichetta. Solo invarianze e regolarizzazioni auto-supervisionate.

Nessun hard-coding semantico. Vietato introdurre obiettivi â€œintelligentiâ€ (es. Sobel, classificatori proxy).

ReplicabilitÃ  assoluta. Config unica, seed fissi, checksum dataset, idempotenza totale.

ModularitÃ . Ogni componente isolato e riutilizzabile: dati, adapters, trainer, metriche.

ğŸ—‚ï¸ Struttura della repository
project/
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ default.yaml                # tutti i parametri (dataset, model, trainer, seedâ€¦)
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                        # file scaricati (read-only)
â”‚   â””â”€â”€ processed/                  # feature preprocessate + stats
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ datasets.py             # download, checksum, split, DataLoader
â”‚   â”‚   â””â”€â”€ augment.py              # augmentazioni per modalitÃ 
â”‚   â”œâ”€â”€ adapters/
â”‚   â”‚   â”œâ”€â”€ vision.py               # CNN percettiva
â”‚   â”‚   â””â”€â”€ audio.py                # CNN su log-mel
â”‚   â”œâ”€â”€ trainers/
â”‚   â”‚   â””â”€â”€ ssl.py                  # SimCLR-lite / VICReg-lite / DINO-lite
â”‚   â”œâ”€â”€ eval/
â”‚   â”‚   â””â”€â”€ metrics.py              # NN-consistency, uniformity, OOS test
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ config.py               # parser YAML, override da CLI
â”‚       â”œâ”€â”€ seed.py                 # seed globali e determinismo
â”‚       â”œâ”€â”€ logging.py              # logger e tracking run
â”‚       â””â”€â”€ dist.py                 # setup multi-GPU opzionale
â”œâ”€â”€ artifacts/
â”‚   â”œâ”€â”€ runs/                       # tensorboard / wandb facoltativo
â”‚   â”œâ”€â”€ ckpts/                      # checkpoint .pt
â”‚   â””â”€â”€ logs/                       # metriche + config salvati
â”œâ”€â”€ README_MILESTONE1.md            # (questo file)
â””â”€â”€ requirements.txt

ğŸ§© Descrizione moduli
datasets.py

Scarica dataset pubblici (CIFAR-10, STL-10 unlabeled, DTD, ESC-50, UrbanSound8K, YESNO).
Per l'audio la configurazione di default usa UrbanSound8K con uno slicing ``train[:2000]``
per mantenere un compromesso tra varietÃ  di classi e dimensione su disco.

Verifica checksum e decomprime.

Preprocess:

Visione: resize 96Ã—96, normalizza mean/std train.

Audio: resample 16 kHz â†’ log-mel (96 bande, 25 ms win, 10 ms hop) â†’ standardizza per banda.

Split deterministico: 95 % train / 5 % val.

Genera file .npz/.pt in data/processed/.

Restituisce DataLoader train/val + OOS (dataset fuori dominio).

augment.py

Augmentazioni fisicamente plausibili ma non â€œsemanticheâ€.

Visione: crop, flip, blur, jitter di colore, solarize soft.

Audio: time-shift, masking soft, jitter ampiezza, rumore bianco debole.
Parametri da config.yaml.

adapters/vision.py

Piccola CNN percettiva:

Conv(3Ã—3, stride=1) â†’ GELU â†’ LayerNorm
Conv(3Ã—3, stride=2) â†’ GELU â†’ LayerNorm
Conv(3Ã—3, stride=2) â†’ GELU â†’ Global AvgPool
Linear(d_inâ†’d) â†’ LayerNorm â†’ L2-norm


Output: vettore â„^d, con d=512 (default).

adapters/audio.py

Pipeline audio:

Input: log-mel [FÃ—T]
Conv(3Ã—3)Ã—3 blocchi â†’ pooling tempo
Linear(d_inâ†’d) â†’ LayerNorm â†’ L2-norm

trainers/ssl.py

Implementa loop self-supervisionato scelto da config:

SimCLR-lite (contrastiva, InfoNCE Ï„).

VICReg-lite (invarianza + varianza + covarianza).

DINO-lite (teacher EMA, opzionale).
Supporta queue negativi, cosine LR, checkpoint.

metrics.py

NN-consistency: % in cui due viste augmentate sono NN reciproche.

Uniformity: varianza delle cosine â†’ 1 = distribuzione uniforme.

OOS-shift: distanza media train â†” OOS.

Retrieval robustness: rank reciproco medio intra-modal.

utils/

Seed, logging, distibuzione multiprocess e parsing config.

ğŸ§® Configurazione (configs/default.yaml)
seed: 42
modalities: { vision: true, audio: true }
data:
  root: ./data
  datasets:
    vision: { name: CIFAR10, checksum: null, options: { train: true } }
    audio:  { name: UrbanSound8K, checksum: null, options: { split: "train[:2000]" } }
  batch_size: { vision: 256, audio: 128 }
  num_workers: 8
model:
  d: 512
  trainer: vicreg    # o simclr / dino
  epochs: 100
  lr: 1e-3
  weight_decay: 0.05
  temperature: 0.1
  augment:
    vision: { crop: 0.8, flip: true, color_jitter: 0.2, blur: 0.3 }
    audio:  { time_shift: 0.2, mask_time: 0.1, noise: 0.01 }
eval:
  every_n_epochs: 10
  metrics: [nn_consistency, uniformity, oos_shift]
logging:
  output_dir: ./artifacts
  use_wandb: false

ğŸš€ Flusso operativo

Preparazione

python -m src.data.datasets --config configs/default.yaml


Scarica, preprocessa, salva statistiche normalizzazione.

Allenamento adapters

python -m src.trainers.ssl --config configs/default.yaml


â†’ produce checkpoint adapters_vision.pt, adapters_audio.pt, log metriche, curve uniformity.

Valutazione non-supervisionata

python -m src.eval.metrics --config configs/default.yaml --ckpt artifacts/ckpts/last.pt


Stampa e salva:

NN-consistency: ...
Uniformity: ...
OOS-shift: ...


Riproduzione

Run con seed identico â†’ stessi risultati (Â±rumore floating).

Tutti i parametri e stats normalizzazione salvati in artifacts/.

ğŸ“Š Risultati attesi e criteri di â€œprontezzaâ€
1. StabilitÃ  e non-collasso
Metrica	Descrizione	Soglia
Uniformity U	Varianza delle cosine intra-batch	0.8 â‰¤ U â‰¤ 1.2 (evita collasso o cluster unici)
Varianza embedding	Var(X) per dimensione	> 0.05
2. Invarianza percettiva
Test	Atteso
NN-consistency (augm â†’ augm)	â‰¥ 0.75
Retrieval OOS (stesso stimolo in dominio diverso)	â‰¥ 0.6 reciprocal rank
3. SeparabilitÃ  percettiva

k-means (k=50) su embedding â†’ silhouette > 0.25 (senza label).

PCA 2D mostra cluster coerenti con pattern visivi/audio (texture, timbro).

4. Robustezza dominio

Media cosine(train,OOS)/cosine(train,train) â‰¥ 0.8 (â‰¤ 20 % degrado).

5. Convergenza

Loss auto-sup stabile (plateau Â±5 % ultime 10 epoche).

Nessun gradiente nan, varianza costante embedding.

âœ… Il sistema Ã¨ â€œpronto per il backboneâ€ quando:

tutte le metriche sopra rientrano nelle soglie per 3 run consecutive (seed diversi);

lâ€™output degli adapters (vision/audio) Ã¨ normalizzato e stabile nel tempo;

il modello ricostruibile con python -m src.data.datasets && python -m src.trainers.ssl
produce embedding equivalenti (cosine > 0.98) rispetto al checkpoint di riferimento.

Solo a questo punto si procede alla Milestone 2 (introduzione del backbone di co-occorrenza e dellâ€™allineamento multimodale).

ğŸ§© Risultato finale della Milestone 1

âœ… Adapters auto-supervisionati che forniscono vettori â„^d coerenti, invariante ad augmentazioni e robusti a shift OOS.

âœ… Pre-processing pipeline deterministica e ri-eseguibile.

âœ… Metriche quantitative di stabilitÃ , invarianza e separabilitÃ  superate.

âœ… Checkpoint e script ri-lanciabili da zero (replicabilitÃ ).

ğŸ“… Step successivi (Milestone 2 â€“ preview)

Introduzione del backbone condiviso (Perceiver-like).

Training su co-occorrenze multimodali (immagineâ†”audio).

Misure di allineamento cross-modal e costruzione concettuale.

ğŸ§­ Note finali

Tutti i componenti devono poter essere lanciati anche separatamente (python -m src.adapters.vision --test â†’ dry run).

Evitare ogni scorciatoia semantica: nessuna label, nessuna loss â€œfurbaâ€.

Ogni parametro modificato va tracciato in config_version.json.

Ogni milestone chiude solo quando i criteri quantitativi vengono soddisfatti.
