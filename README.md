# ðŸ§  Modulo Percettivo Basato su Co-Occorrenze Temporali

Questa repository contiene la prima milestone del progetto **Arca** ricalibrata per
rispettare i vincoli imposti sul nuovo schema percettivo. Il focus Ã¨ la costruzione di
adapters visivi e acustici che imparano esclusivamente da co-occorrenze ripetute nel
tempo, senza utilizzare loss contrastive nÃ© compiti predittivi cross-modali.

## Principi non negoziabili

- **Nessun contrastive loss.** Lâ€™apprendimento si basa su correlazioni temporali
  accumulate e non su InfoNCE/SimCLR/affini.
- **Nessuna predizione cross-modale esplicita.** Le associazioni emergono dalla
  co-presenza di stimoli visivi e sonori in clip temporali ripetuti.
- **TemporalitÃ  multi-scala.** Ogni associazione Ã¨ pesata da un kernel temporale che
  combina piÃ¹ scale di decadimento, non solo dalla sincronia istantanea.
- **Parsing object-centric opzionale.** Gli oggetti vengono segmentati in maniera
  deterministica, senza supervisioni esterne o obiettivi predittivi.
- **Determinismo e riproducibilitÃ .** Seed fissati, manifest JSON con tutti i fattori
  latenti e pipeline idempotente.

## Dataset sintetico controllato

Il dataset Ã¨ generato on-the-fly da `src/data/datasets.py` e produce clip da 1.5â€“3 s con
1â€“3 oggetti e 1â€“3 eventi sonori. Ogni clip viene descritta nel manifest JSON con i
fattori latenti (forma, colore, scala, posizione, tempi).

- **Componenti visivi**: forme semplici (quadrato, cerchio, triangolo) renderizzate su
  sfondo nero con jitter controllato su colore, scala e posizione.
- **Componenti audio**: onde sinusoidali/saw/square generate sinteticamente, parametrizzate
  per pitch, timbro, durata e ampiezza.
- **Mapping deterministico**:
  - colore â†” timbre (es. rosso â†’ saw, verde â†’ square, blu â†’ sine)
  - forma â†” intervallo (0/4/7 semitoni sullâ€™oscillatore base)
  - scala â†” loudness (ampiezza proporzionale alla dimensione visiva)
  - numero di oggetti â†” polifonia (eventi audio sovrapposti)
- **Split composizionale**: alcune combinazioni forma/colore vengono tenute fuori dal
  training e appaiono solo in validation/test per il test zero-shot.
- **Manifest**: `data/processed/cooccurrence_manifest.json` elenca ogni clip con i
  metadati, mentre `cooccurrence_stats.json` raccoglie valori globali e kernel temporali.

## Pipeline

1. **Generazione dati** (deterministica):

   ```bash
   python -m src.data.datasets --config configs/default.yaml
   ```

   Produce tre file `.npz` (`train/val/test`) con tensori oggetto/evento giÃ  allineati,
   il manifest JSON e lo stats JSON.

2. **Training adapters** basato su co-occorrenza temporale:

   ```bash
   python -m src.trainers.ssl --config configs/default.yaml
   ```

   - Gli embeddings visivi e audio vengono calcolati per ogni oggetto/evento.
   - La loro similaritÃ  viene avvicinata al kernel temporale multi-scala precomputato.
   - PenalitÃ  di varianza garantiscono rappresentazioni non collassate.
   - Il training produce un checkpoint unico contenente entrambi gli adapters e la storia
     delle loss.

## Architettura

```
project/
â”œâ”€â”€ configs/default.yaml           # configurazione globale (seed, dataset, trainer)
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                       # placeholder (nessun download necessario)
â”‚   â””â”€â”€ processed/                 # dataset sintetico + manifest/stats
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data/datasets.py           # generatore sintetico co-occorrenza
â”‚   â”œâ”€â”€ adapters/vision.py         # encoder CNN object-centric
â”‚   â”œâ”€â”€ adapters/audio.py          # encoder 1D log-mel
â”‚   â””â”€â”€ trainers/ssl.py            # trainer co-occorrenza senza loss contrastive
â””â”€â”€ artifacts/
    â”œâ”€â”€ ckpts/                     # checkpoint finali
    â””â”€â”€ logs/                      # log + curva loss
```

## Dettagli del trainer

- **Input**: batch di clip con tensori `vision [B, O, 3, 96, 96]` e `audio [B, O, 64, T]`,
  maschere per oggetti/eventi e kernel temporale `[B, O, O]`.
- **Obiettivo**: minimizzare lâ€™errore quadratico tra la similaritÃ  cosine (mappata in [0,1])
  e il kernel temporale normalizzato, garantendo che le correlazioni rispettino le scale
  temporali fornite.
- **Regolarizzazione**: varianza minima per ciascun embedding e bilanciamento delle medie
  di similaritÃ  per evitare collasso o allineamenti spurii.
- **Output**: `artifacts/ckpts/associative_adapters.pt` con pesi degli adapters e stato
  dellâ€™optimizer.

## Configurazione

`configs/default.yaml` controlla:

- Numero di clip per split e combinazioni holdout.
- Ampiezza del jitter su colore/pitch/loudness/posizione/scala.
- Parametri del trainer: numero di epoche, learning rate, weight decay,
  soglia di varianza e scale del kernel temporale.

Modificando il file Ã¨ possibile riprodurre esperimenti o creare nuovi manifest
composizionali mantenendo il determinismo (seed globale).

## Stato attuale

- âœ… Dataset sintetico minimalista con mapping deterministico e manifest dei fattori.
- âœ… Trainer che rispetta il vincolo di apprendimento per co-occorrenza senza loss
  contrastive nÃ© compiti predittivi cross-modali.
- âœ… Pipeline completamente riproducibile (stessi seed â†’ stessi file, stessi risultati).

Questo modulo rappresenta la base sensoriale per step successivi del progetto Arca,
in cui verrÃ  introdotto un backbone multimodale piÃ¹ ampio.


## Esportare l'intero progetto

Per creare rapidamente un archivio ZIP contenente l'intera repository (inclusi README e
configurazioni) Ã¨ disponibile uno script dedicato:

```bash
python scripts/export_project.py
```

Il comando genera un file `zip` all'interno della cartella `exports/` con un timestamp nel
nome. Per salvare l'archivio in un percorso personalizzato Ã¨ possibile usare l'opzione
`--output`:

```bash
python scripts/export_project.py --output /percorso/destinazione/arca.zip
```

Le cartelle temporanee come `.git`, `exports/` e `__pycache__/` sono escluse
automaticamente dall'archivio.
